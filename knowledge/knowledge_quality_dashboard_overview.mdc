# Rule: Quality Management Dashboard Overview

## Description
This rule outlines the concepts and components of a quality management dashboard for AI-driven development projects, as described in `project_management_guide.md`. It covers integration with GitHub Actions, elements of a custom dashboard, and CI/CD pipeline integration.

## Scope
- Projects aiming to monitor and improve code quality, test coverage, and development process efficiency, especially where AI agents are involved.

## Key Concepts

### 1. Purpose of a Quality Management Dashboard
- **Visualize Quality Metrics:** Provide a centralized view of key quality indicators.
- **Track Trends:** Monitor improvements or regressions in code quality and testing over time.
- **Identify Bottlenecks:** Highlight areas needing attention, such as low test coverage or high code complexity.
- **Support Decision Making:** Offer data-driven insights for PMs and development teams (including AI agents) to make informed decisions about quality improvement efforts.
- **Enhance Transparency:** Make quality status visible to all stakeholders.

### 2. Integration with GitHub Actions for Automation
Leverage GitHub Actions to automate the collection and reporting of quality metrics:
- **Contribution Analysis (via GitHub Insights or custom scripts):**
    - Track commit history, including AI agent contributions.
    - Analyze Pull Request (PR) statistics: volume, merge time, review comments.
    - Monitor AI agent involvement in Issue resolution and PRs (e.g., auto-closed issues, PRs generated by AI).
- **Code Review Statistics:**
    - Frequency and duration of human reviews.
    - Number of AI-generated PRs修正 (modifications) based on human feedback.
- **Automated Reporting:** Schedule regular generation and publication of quality reports (e.g., daily or on every push to main).
- **Example Workflow:** See `reference_github_actions_quality_workflow.yml`.

### 3. Building a Custom Quality Dashboard
While GitHub provides some insights, a custom dashboard can offer a more tailored view. Key components include:

#### a. Test Coverage Dashboard
- **Metrics:** Overall coverage percentage, coverage per module/component, untested files/lines.
- **Data Source:** Output from coverage tools like `pytest-cov` (e.g., `coverage.xml`).
- **Features:**
    - **Daily/Trend Reports:** Display current coverage and historical trends (e.g., line graph of coverage over time).
    - **Component Breakdown:** Show coverage for different parts of the application to identify low-coverage areas.
    - **Threshold Alerts:** (Optional) Notify if coverage drops below a defined threshold.

#### b. Code Quality Dashboard
- **Metrics:** Static analysis warnings/errors, code complexity (e.g., cyclomatic complexity), technical debt indicators.
- **Data Sources:** Output from linters (Flake8, Pylint), static analyzers (Mypy), complexity tools, and custom scripts for technical debt (e.g., counting TODO/FIXME comments, identifying duplicate code).
- **Features:**
    - **Aggregated Reports:** Summarize findings from various analysis tools.
    - **Complexity Hotspots:** Identify functions or classes with high complexity that might need refactoring.
    - **Technical Debt Tracking:** Visualize the amount and type of identified technical debt and track its reduction over time.
    - **Code Smells:** List common code smells detected.

### 4. CI/CD Pipeline Integration
- **Automate Data Collection:** Integrate quality metric collection directly into the CI/CD pipeline.
    - Run tests with coverage on every commit/PR.
    - Execute static analysis tools as part of the pipeline.
- **Automate Report Generation:** Use scripts (e.g., Python scripts) within the CI/CD pipeline to process tool outputs and generate dashboard data or HTML reports.
    - Example Script Outline: See `reference_quality_dashboard_script_example.py`.
- **Automate Report Deployment:** Publish generated reports to a static site (e.g., GitHub Pages) or a dedicated dashboarding tool.

## Outputs
- A comprehensive understanding of how to set up and utilize a quality management dashboard.
- Actionable insights derived from the dashboard to improve code quality and development processes.

## Success Criteria
- The quality dashboard provides a clear, accurate, and up-to-date view of relevant quality metrics.
- Data collection and report generation are automated through CI/CD and GitHub Actions.
- The dashboard is actively used by the PM and AI agents to monitor quality and guide improvements.
- Trends in quality metrics show improvement over time or highlight areas for focused effort.

## 品質ダッシュボード概要

品質ダッシュボードは、プロジェクトの健全性・コード品質・セキュリティ状況・AIエージェントの活動を可視化し、データに基づく意思決定・継続的改善を支援するためのツールです。

### 主な目的
- 透明性の確保
- 早期問題検出
- 継続的改善の促進
- PMによる戦略的判断支援

### 主要な利用シーン
- 日々の開発・運用状況の把握
- 品質・セキュリティ指標のトレンド監視
- 問題発生時の原因分析・改善活動

詳細は @knowledge_quality_dashboard_metrics.mdc, @knowledge_quality_dashboard_architecture.mdc も参照。
